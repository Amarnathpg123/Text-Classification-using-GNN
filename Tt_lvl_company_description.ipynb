{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tt_lvl_company_description.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "ZueMWfUfJpxh",
        "qD_SSAB0KILz",
        "cTHiKOtkdlj_",
        "Ar3OkCxYd3jy",
        "Hny2fIC6dSMb",
        "dfFSmbFlsS_s",
        "tOe2Y-iGeA5U",
        "Z9oxoQ26CVyI"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm6F1QpE4OmG"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUM75wdGLmdY"
      },
      "source": [
        "import pandas as pd\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\r\n",
        "import numpy as np\r\n",
        "from sklearn.metrics import f1_score, roc_auc_score\r\n",
        "from time import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSJWthfdMDZ1"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVqp5Gxxmu1M"
      },
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZueMWfUfJpxh"
      },
      "source": [
        "## preparing train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F78Dmcxl2G6"
      },
      "source": [
        "df = pd.read_excel('/content/gdrive/MyDrive/GNN Intern/Training_Data.xlsx')\r\n",
        "print(df.shape)\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hYxrKQ6mBIq"
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0NyAAjCmLlG"
      },
      "source": [
        "df[df[\"Business Description\"].isnull() == True]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WISofsilmPmG"
      },
      "source": [
        "df['Business Description'] = np.where(df[\"Business Description\"].isnull() == True,df[\"Company Name\"],df[\"Business Description\"])\r\n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1Ir8DUPmSW2"
      },
      "source": [
        "df.drop_duplicates(keep=False,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCEp_5femUhm"
      },
      "source": [
        "df.info()\r\n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UImXIhpEmWkf"
      },
      "source": [
        "df[\"Business Description\"].str.len().describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQbR_kIgmYtH"
      },
      "source": [
        "df[\"Business Description\"].str.len().plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOyFwQq4mbwW"
      },
      "source": [
        "df[\"Business Description\"].str.len().plot.box()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPu7qE9Fmiy9"
      },
      "source": [
        "classes = {i:typ for i,typ in enumerate(df.iloc[:,2].unique())}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wV0ynS7mejR"
      },
      "source": [
        "df.iloc[:,1] = df.iloc[:,1].str.lower()\r\n",
        "df.iloc[:,2] = df.iloc[:,2].str.lower()\r\n",
        "df.columns = df.columns.str.strip()\r\n",
        "df.columns = df.columns.str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SWTAIPFmhCG"
      },
      "source": [
        "df.drop('company name',1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feyLI4rlupoz"
      },
      "source": [
        "dum = df['industry classification tag']\r\n",
        "df.drop(columns='industry classification tag',inplace=True)\r\n",
        "df = pd.concat([dum,df],axis=1)\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1fowegIIZ99"
      },
      "source": [
        "df.columns = ['','']\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3J-lQfkzIvOA"
      },
      "source": [
        "train = df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0vAxiJsSN54"
      },
      "source": [
        "df = pd.read_excel('/content/gdrive/MyDrive/GNN Intern/Testing_Data.xlsx')\r\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENtjDo3EShBr"
      },
      "source": [
        "df.columns = ['0','']\r\n",
        "df.drop(columns='0',axis=1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-XqPnmuTrnZ"
      },
      "source": [
        "df.iloc[:,0] = df.iloc[:,0].str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2faaaIGQTyif"
      },
      "source": [
        "test = df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNuQxXOdKP_E"
      },
      "source": [
        "train.to_csv('/content/gdrive/MyDrive/GNN Intern/train.csv',index=False)\r\n",
        "test.to_csv('/content/gdrive/MyDrive/GNN Intern/test.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkHQbjjUE31t"
      },
      "source": [
        "del df, test, train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD_SSAB0KILz"
      },
      "source": [
        "# Tokenizer and embedding matrix from pretrained embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFsBGDphKRHz"
      },
      "source": [
        "class Tokenizer:\r\n",
        "  def __init__(self, pretrained_file):\r\n",
        "    self.pretrained_file = pretrained_file\r\n",
        "    self.str_to_int = dict() #for fast lookups\r\n",
        "    self.int_to_str = dict()\r\n",
        "\r\n",
        "    self.padding = '#pad#'\r\n",
        "    self.rand_str = '#rand#'\r\n",
        "    self.embedding_matrix = list()\r\n",
        "\r\n",
        "    #preparing embedding matrix\r\n",
        "    with open(pretrained_file,'r', encoding='utf8') as f:\r\n",
        "      for i, line in enumerate(f):\r\n",
        "        values = line.split()\r\n",
        "        self.str_to_int[values[0]] = i\r\n",
        "        self.int_to_str[i] = values[0]\r\n",
        "        self.embedding_matrix.append([float(v) for v in values[1:]])\r\n",
        "    i += 1\r\n",
        "    self.str_to_int[self.rand_str] = i\r\n",
        "    self.int_to_str[i] = self.rand_str\r\n",
        "    self.embedding_matrix.append(np.random.rand(len(self.embedding_matrix[0])))\r\n",
        "\r\n",
        "    i += 1\r\n",
        "    self.str_to_int[self.padding] = i\r\n",
        "    self.int_to_str[i] = self.padding\r\n",
        "    self.embedding_matrix.append(np.zeros(len(self.embedding_matrix[0])))\r\n",
        "\r\n",
        "    self.embedding_matrix = np.array(self.embedding_matrix).astype(np.float32)\r\n",
        "\r\n",
        "  def encode(self, sentence):\r\n",
        "    if len(sentence): list(sentence)\r\n",
        "    else: sentence = sentence.split(\" \")\r\n",
        "\r\n",
        "    encoded_sentence = list()\r\n",
        "    for word in sentence:\r\n",
        "      encoded_sentence.append(self.str_to_int.get(word,self.str_to_int[self.rand_str]))\r\n",
        "    return encoded_sentence\r\n",
        "\r\n",
        "  def decode(self,en_sentence):\r\n",
        "    if type(en_sentence) == list:\r\n",
        "      sentence = list()\r\n",
        "      for en_word in en_sentence:\r\n",
        "        sentence.append(self.int_to_str[en_word])\r\n",
        "      return senetnce\r\n",
        "\r\n",
        "  def embedding(self, en_sentence):\r\n",
        "    return self.embedding_matrix[np.array(en_sentence)]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fYPyLSzJa6e"
      },
      "source": [
        "tokenizer = Tokenizer('/content/gdrive/MyDrive/GNN Intern/glove.6B.300d.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTHiKOtkdlj_"
      },
      "source": [
        "# Preparing Train, Validation and Test Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdtKe7SpQEKk"
      },
      "source": [
        "def create_neighbor_set(node_set, p=3):\r\n",
        "  sequence_length = len(node_set)\r\n",
        "  neighbor_set = []\r\n",
        "  for i in range(sequence_length):\r\n",
        "      neighbor = []\r\n",
        "      for j in range(-p, p+1):\r\n",
        "          if 0 <= i + j < sequence_length:\r\n",
        "              neighbor.append(node_set[i+j])\r\n",
        "      neighbor_set.append(neighbor)\r\n",
        "  return neighbor_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sTrsPuyU5Ta"
      },
      "source": [
        "class GNN_Dataset(Dataset):\r\n",
        "    def __init__(self, node_sets, neighbor_sets, public_edge_mask, labels):\r\n",
        "      super(GNN_Dataset).__init__()\r\n",
        "      self.node_sets = node_sets\r\n",
        "      self.neighbor_sets = neighbor_sets\r\n",
        "      self.public_edge_mask = public_edge_mask\r\n",
        "      self.labels = labels\r\n",
        "\r\n",
        "    def __getitem__(self, i):\r\n",
        "      if self.labels:\r\n",
        "        return torch.LongTensor(self.node_sets[i]), \\\r\n",
        "              torch.nn.utils.rnn.pad_sequence([torch.LongTensor(neighbor) for neighbor in self.neighbor_sets[i]], batch_first=True, padding_value=1), \\\r\n",
        "              self.public_edge_mask[torch.LongTensor(self.node_sets[i]).unsqueeze(-1).repeat(1, torch.nn.utils.rnn.pad_sequence([torch.LongTensor(neighbor) for neighbor in self.neighbor_sets[i]], batch_first=True, padding_value=1).shape[-1]), \\\r\n",
        "                                    torch.nn.utils.rnn.pad_sequence([torch.LongTensor(neighbor) for neighbor in self.neighbor_sets[i]], batch_first=True, padding_value=1)], \\\r\n",
        "              torch.FloatTensor(self.labels[i])\r\n",
        "      else:\r\n",
        "        return torch.LongTensor(self.node_sets[i]), \\\r\n",
        "              torch.nn.utils.rnn.pad_sequence([torch.LongTensor(neighbor) for neighbor in self.neighbor_sets[i]], batch_first=True, padding_value=1), \\\r\n",
        "              self.public_edge_mask[torch.LongTensor(self.node_sets[i]).unsqueeze(-1).repeat(1, torch.nn.utils.rnn.pad_sequence([torch.LongTensor(neighbor) for neighbor in self.neighbor_sets[i]], batch_first=True, padding_value=1).shape[-1]), \\\r\n",
        "                                    torch.nn.utils.rnn.pad_sequence([torch.LongTensor(neighbor) for neighbor in self.neighbor_sets[i]], batch_first=True, padding_value=1)]\r\n",
        "\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "      return len(self.node_sets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArLgLxV4Q-xu"
      },
      "source": [
        "class GNN_Dataset_class:\r\n",
        "  def __init__(self, train_filename, test_filename, tokenizer, MAX_LENGTH=70, p=3, min_freq=2, train_validation_split=0.8):\r\n",
        "    self.train_filename = train_filename\r\n",
        "    self.test_filename = test_filename\r\n",
        "    self.tokenizer = tokenizer\r\n",
        "    self.MAX_LENGTH = MAX_LENGTH\r\n",
        "    self.p = p\r\n",
        "    self.min_freq = min_freq\r\n",
        "    self.train_validation_split = train_validation_split\r\n",
        "\r\n",
        "    self.train_data = pd.read_csv(self.train_filename, header=None)\r\n",
        "    self.train_data.dropna(0,inplace=True)\r\n",
        "    self.test_data = pd.read_csv(self.test_filename, header=None)\r\n",
        "    self.test_data.dropna(0,inplace=True)\r\n",
        "\r\n",
        "    self.str_to_int = {'#rand#': 0, '#pad#': 1} \r\n",
        "    self.int_to_str = {0: '#rand#', 1: '#pad#'}\r\n",
        "    self.vocab_count = len(self.str_to_int)\r\n",
        "    self.embedding_matrix = None\r\n",
        "    self.label_dict = dict(zip(self.train_data[0].unique(), pd.get_dummies(self.train_data[0].unique()).values.tolist()))\r\n",
        "\r\n",
        "    self.train_dataset, self.validation_dataset = random_split(self.train_data.to_numpy(), [int(len(self.train_data) * train_validation_split), len(self.train_data) - int(len(self.train_data) * train_validation_split)])\r\n",
        "    self.test_dataset = self.test_data.to_numpy()\r\n",
        "\r\n",
        "    self.build_vocab()\r\n",
        "\r\n",
        "    self.train_dataset, self.validation_dataset, self.test_dataset, self.edge_stat, self.public_edge_mask = self.prepare_dataset()\r\n",
        "\r\n",
        "  def build_vocab(self):\r\n",
        "    scrap = [\".\",\",\",\";\",\"&\",\"'s\", \":\", \"?\", \"!\",\"(\",\")\",\\\r\n",
        "            \"'\",\"`\",\"''\",\"\\\"\",\"“\",\" \",\"'m\",\"'no\",\"***\",\"--\",\"...\",\"[\",\"]\",\"{\",\"}\",\"~\",\"@\",\"#\",\"$\",\"%\",\"^\",\"*\",\"/\",\"<\",\">\",\"+\",\"-\",\"=\"]\r\n",
        "    vocab_list = [sentence.split(' ') for _, sentence in self.train_dataset]\r\n",
        "    unique_vocab = [] \r\n",
        "    for vocab in vocab_list:\r\n",
        "      if vocab not in scrap: unique_vocab.extend(vocab)\r\n",
        "    unique_vocab = list(set(unique_vocab))\r\n",
        "    for vocab in unique_vocab:\r\n",
        "      if vocab in self.tokenizer.str_to_int.keys():\r\n",
        "        self.str_to_int[vocab] = self.vocab_count\r\n",
        "        self.int_to_str[self.vocab_count] = vocab\r\n",
        "        self.vocab_count += 1\r\n",
        "    self.embedding_matrix = self.tokenizer.embedding(self.tokenizer.encode(list(self.str_to_int.keys())))\r\n",
        "\r\n",
        "  def prepare_dataset(self):\r\n",
        "    node_sets = [[self.str_to_int.get(vocab, 0) for vocab in sentence.strip().split(' ')][:self.MAX_LENGTH] for _, sentence in self.train_dataset] \r\n",
        "    neighbor_sets = [create_neighbor_set(node_set, p=self.p) for node_set in node_sets]\r\n",
        "    labels = [self.label_dict[label] for label, _ in self.train_dataset]\r\n",
        "\r\n",
        "    edge_stat, public_edge_mask = self.build_public_edge_mask(node_sets, neighbor_sets, min_freq=self.min_freq)\r\n",
        "\r\n",
        "    train_dataset = GNN_Dataset(node_sets, neighbor_sets, public_edge_mask, labels)\r\n",
        "\r\n",
        "    node_sets = [[self.str_to_int.get(vocab, 0) for vocab in sentence.strip().split(' ')][:self.MAX_LENGTH] for _, sentence in self.validation_dataset]\r\n",
        "    neighbor_sets = [create_neighbor_set(node_set, p=self.p) for node_set in node_sets]\r\n",
        "    labels = [self.label_dict[label] for label, _ in self.validation_dataset]\r\n",
        "    validation_dataset = GNN_Dataset(node_sets, neighbor_sets, public_edge_mask, labels)\r\n",
        "\r\n",
        "    node_sets = [[self.str_to_int.get(vocab, 0) for vocab in sentence.item().strip().split(' ')][:self.MAX_LENGTH] for sentence in self.test_dataset]\r\n",
        "    neighbor_sets = [create_neighbor_set(node_set, p=self.p) for node_set in node_sets]\r\n",
        "    test_dataset = GNN_Dataset(node_sets, neighbor_sets, public_edge_mask, labels=None)\r\n",
        "\r\n",
        "    return train_dataset, validation_dataset, test_dataset, edge_stat, public_edge_mask\r\n",
        "\r\n",
        "  def build_public_edge_mask(self, node_sets, neighbor_sets, min_freq=2):\r\n",
        "    edge_stat = torch.zeros(self.vocab_count, self.vocab_count)\r\n",
        "    for node_set, neighbor_set in zip(node_sets, neighbor_sets):\r\n",
        "      for neighbor in neighbor_set:\r\n",
        "        for to_node in neighbor:\r\n",
        "          edge_stat[node_set, to_node] += 1\r\n",
        "    public_edge_mask = edge_stat < min_freq\r\n",
        "    return edge_stat, public_edge_mask\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGzKjxa8LTHk"
      },
      "source": [
        "dataset = GNN_Dataset_class(train_filename='/content/gdrive/MyDrive/GNN Intern/train.csv',\r\n",
        "                                   test_filename='/content/gdrive/MyDrive/GNN Intern/test.csv',\r\n",
        "                                   tokenizer=tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyas92Qkem4X"
      },
      "source": [
        "def pad_custom_sequence(sequences):\r\n",
        "  node_sets_sequence = []\r\n",
        "  neighbor_sets_sequence = []\r\n",
        "  public_edge_mask_sequence = []\r\n",
        "  label_sequence = []\r\n",
        "  for node_sets, neighbor_sets, public_edge_mask, label in sequences:\r\n",
        "    node_sets_sequence.append(node_sets)\r\n",
        "    neighbor_sets_sequence.append(neighbor_sets)\r\n",
        "    public_edge_mask_sequence.append(public_edge_mask)\r\n",
        "    label_sequence.append(label)\r\n",
        "  node_sets_sequence = torch.nn.utils.rnn.pad_sequence(node_sets_sequence, batch_first=True, padding_value=1)\r\n",
        "  neighbor_sets_sequence, _ = padding_tensor(neighbor_sets_sequence)\r\n",
        "  public_edge_mask_sequence, _ = padding_tensor(public_edge_mask_sequence)\r\n",
        "  label_sequence = torch.nn.utils.rnn.pad_sequence(label_sequence, batch_first=True, padding_value=1)\r\n",
        "  return node_sets_sequence, neighbor_sets_sequence, public_edge_mask_sequence, label_sequence\r\n",
        "\r\n",
        "def pad_custom_for_test(sequences):\r\n",
        "  node_sets_sequence = []\r\n",
        "  neighbor_sets_sequence = []\r\n",
        "  public_edge_mask_sequence = []\r\n",
        "  for node_sets, neighbor_sets, public_edge_mask in sequences:\r\n",
        "    node_sets_sequence.append(node_sets)\r\n",
        "    neighbor_sets_sequence.append(neighbor_sets)\r\n",
        "    public_edge_mask_sequence.append(public_edge_mask)\r\n",
        "  node_sets_sequence = torch.nn.utils.rnn.pad_sequence(node_sets_sequence, batch_first=True, padding_value=1)\r\n",
        "  neighbor_sets_sequence, _ = padding_tensor(neighbor_sets_sequence)\r\n",
        "  public_edge_mask_sequence, _ = padding_tensor(public_edge_mask_sequence)\r\n",
        "  return node_sets_sequence, neighbor_sets_sequence, public_edge_mask_sequence\r\n",
        "  \r\n",
        "\r\n",
        "def padding_tensor(sequences, padding_idx=1):\r\n",
        "  num = len(sequences)\r\n",
        "  max_len_0 = max([s.shape[0] for s in sequences])\r\n",
        "  max_len_1 = max([s.shape[1] for s in sequences])\r\n",
        "  out_dims = (num, max_len_0, max_len_1)\r\n",
        "  out_tensor = sequences[0].data.new(*out_dims).fill_(padding_idx)\r\n",
        "  for i, tensor in enumerate(sequences):\r\n",
        "    len_0 = tensor.size(0)\r\n",
        "    len_1 = tensor.size(1)\r\n",
        "    out_tensor[i, :len_0, :len_1] = tensor\r\n",
        "  mask = out_tensor == padding_idx\r\n",
        "  return out_tensor, mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdBUsDYYewnj"
      },
      "source": [
        "train_loader = DataLoader(dataset.train_dataset, batch_size=32, shuffle=True, collate_fn=pad_custom_sequence)\r\n",
        "validation_loader = DataLoader(dataset.validation_dataset, batch_size=32, shuffle=True, collate_fn=pad_custom_sequence)\r\n",
        "test_loader = DataLoader(dataset.test_dataset, batch_size=32, collate_fn=pad_custom_for_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar3OkCxYd3jy"
      },
      "source": [
        "# Text Level GNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9_1SyhKe-bt"
      },
      "source": [
        "class MessagePassing(nn.Module):\r\n",
        "  def __init__(self, vertice_count, input_size, out_size, dropout_rate=0, padding_idx=1):\r\n",
        "    super(MessagePassing, self).__init__()\r\n",
        "    self.vertice_count = vertice_count \r\n",
        "    self.input_size = input_size \r\n",
        "    self.out_size = out_size \r\n",
        "    self.dropout_rate = dropout_rate\r\n",
        "    self.padding_idx = padding_idx\r\n",
        "    self.information_rate = nn.Parameter(torch.rand(self.vertice_count, 1)) \r\n",
        "    self.linear = nn.Linear(self.input_size, self.out_size)\r\n",
        "    self.dropout = nn.Dropout(self.dropout_rate)\r\n",
        "\r\n",
        "  def forward(self, node_sets, embedded_node, edge_weight, embedded_neighbor_node):\r\n",
        "    tmp_tensor = (edge_weight.view(-1, 1) * embedded_neighbor_node.view(-1, self.input_size)).view(embedded_neighbor_node.shape) \r\n",
        "    tmp_tensor = tmp_tensor.masked_fill(tmp_tensor == 0, -1e18) \r\n",
        "    tmp_tensor = self.dropout(tmp_tensor)\r\n",
        "    M = tmp_tensor.max(dim=2)[0] \r\n",
        "    information_rate = self.information_rate[node_sets] \r\n",
        "    information_rate = information_rate.masked_fill((node_sets == self.padding_idx).unsqueeze(-1), 1) \r\n",
        "    embedded_node = (1 - information_rate) * M + information_rate * embedded_node \r\n",
        "    sum_embedded_node = embedded_node.sum(dim=1) \r\n",
        "    x = F.relu(self.linear(sum_embedded_node)) \r\n",
        "    #x = self.dropout(x)\r\n",
        "    y = F.softmax(x, dim=1)\r\n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnC3uQD7fjNG"
      },
      "source": [
        "class TextLevelGNN(nn.Module):\r\n",
        "  def __init__(self, pretrained_embeddings, out_size, dropout_rate=0, padding_idx=1):\r\n",
        "    super(TextLevelGNN, self).__init__()\r\n",
        "    self.out_size = out_size # c\r\n",
        "    self.padding_idx = padding_idx\r\n",
        "    self.weight_matrix = nn.Parameter(torch.randn(pretrained_embeddings.shape[0], pretrained_embeddings.shape[0])) \r\n",
        "    self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False, padding_idx=self.padding_idx) \r\n",
        "    self.message_passing = MessagePassing(vertice_count=pretrained_embeddings.shape[0], input_size=pretrained_embeddings.shape[1], out_size=self.out_size, dropout_rate=dropout_rate, padding_idx=self.padding_idx) \r\n",
        "    self.public_edge_weight = nn.Parameter(torch.randn(1, 1)) \r\n",
        "\r\n",
        "  def forward(self, node_sets, neighbor_sets, public_edge_mask):\r\n",
        "    embedded_node = self.embedding(node_sets)\r\n",
        "    edge_weight = model.weight_matrix[node_sets.unsqueeze(2).repeat(1, 1, neighbor_sets.shape[-1]), neighbor_sets] \r\n",
        "    a = edge_weight * ~public_edge_mask \r\n",
        "    b = self.public_edge_weight.unsqueeze(2).expand(1, public_edge_mask.shape[-2], public_edge_mask.shape[-1]) * public_edge_mask # (batch_size, max_sentence_length, max_neighbor_count)\r\n",
        "    edge_weight = a + b \r\n",
        "    embedded_neighbor_node = self.embedding(neighbor_sets)\r\n",
        "\r\n",
        "    # Apply mask to edge_weight, to mask and cut-off any relationships to the padding nodes\r\n",
        "    edge_weight = edge_weight.masked_fill((node_sets.unsqueeze(2).repeat(1, 1, neighbor_sets.shape[-1]) == self.padding_idx) | (neighbor_sets == self.padding_idx), 0) # (batch_size, max_sentence_length, max_neighbor_count)\r\n",
        "    x = self.message_passing(node_sets, embedded_node, edge_weight, embedded_neighbor_node) # (batch_size, c)\r\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihEzhMnOe2xU"
      },
      "source": [
        "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hny2fIC6dSMb"
      },
      "source": [
        "# Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYNCMf36h0a_"
      },
      "source": [
        "model = TextLevelGNN(pretrained_embeddings=torch.tensor(dataset.embedding_matrix), out_size = 62,dropout_rate=0).to(device)\r\n",
        "criterion = nn.BCELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBfLi8dBh9jv"
      },
      "source": [
        "lr = 1e-2\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\r\n",
        "ni = nf = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDlxj5WuSMcV"
      },
      "source": [
        "\"\"\"\r\n",
        "model_save_name = 'Tt_lvl_company description.pt'\r\n",
        "path = F\"/content/gdrive/My Drive/GNN Intern/saved_models/{model_save_name}\"\r\n",
        "model.load_state_dict(torch.load(path, map_location=device))\r\n",
        "model.to(device)\r\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uk_abPvjBl4"
      },
      "source": [
        "ni += nf \r\n",
        "nf += 1\r\n",
        "for epoch in range(ni,nf):\r\n",
        "  model.train()\r\n",
        "  train_loss = 0\r\n",
        "  train_correct_items = 0\r\n",
        "  previous_epoch_timestamp = time()\r\n",
        "\r\n",
        "  if (epoch+1) % 5 == 0:\r\n",
        "    if epoch: \r\n",
        "      lr *= 0.95\r\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\r\n",
        "\r\n",
        "  train_preds = []\r\n",
        "  validation_preds = []\r\n",
        "  if epoch == 0:\r\n",
        "    train_labels = []\r\n",
        "    validation_labels = []\r\n",
        "  for i, (node_sets, neighbor_sets, public_edge_masks, labels) in enumerate(train_loader):\r\n",
        "    node_sets = node_sets.to(device)\r\n",
        "    neighbor_sets = neighbor_sets.to(device)\r\n",
        "    public_edge_masks = public_edge_masks.to(device)\r\n",
        "    labels = labels.to(device)\r\n",
        "    if epoch == 0: train_labels.append(labels.argmax(dim=1))\r\n",
        "    prediction = model(node_sets, neighbor_sets, public_edge_masks)\r\n",
        "    train_preds.append(prediction)\r\n",
        "    loss = criterion(prediction, labels).to(device)\r\n",
        "    optimizer.zero_grad()\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "    train_loss += loss.item()\r\n",
        "    train_correct_items += (prediction.argmax(dim=1) == labels.argmax(dim=1)).sum().item()\r\n",
        "  train_accuracy = train_correct_items / len(dataset.train_dataset)\r\n",
        "\r\n",
        "  model.eval()\r\n",
        "  validation_loss = 0\r\n",
        "  validation_correct_items = 0\r\n",
        "  for i, (node_sets, neighbor_sets, public_edge_masks, labels) in enumerate(validation_loader):\r\n",
        "    node_sets = node_sets.to(device)\r\n",
        "    neighbor_sets = neighbor_sets.to(device)\r\n",
        "    public_edge_masks = public_edge_masks.to(device)\r\n",
        "    labels = labels.to(device)\r\n",
        "    if epoch == 0: validation_labels.append(labels.argmax(dim=1))\r\n",
        "    prediction = model(node_sets, neighbor_sets, public_edge_masks)\r\n",
        "    validation_preds.append(prediction)\r\n",
        "    loss = criterion(prediction, labels).to(device)\r\n",
        "    validation_loss += loss.item()\r\n",
        "    validation_correct_items += (prediction.argmax(dim=1) == labels.argmax(dim=1)).sum().item()\r\n",
        "  validation_accuracy = validation_correct_items / len(dataset.validation_dataset)\r\n",
        "\r\n",
        "  print(f'Epoch: {epoch+1}, Training Loss: {train_loss:.4f}, Validation Loss: {validation_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {validation_accuracy:.4f}, Time Used: {time()-previous_epoch_timestamp:.2f}s')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "386wRHb-ZDMQ"
      },
      "source": [
        "model.eval()\r\n",
        "test_preds = []\r\n",
        "for i, (node_sets, neighbor_sets, public_edge_masks) in enumerate(test_loader):\r\n",
        "  node_sets = node_sets.to(device)\r\n",
        "  neighbor_sets = neighbor_sets.to(device)\r\n",
        "  public_edge_masks = public_edge_masks.to(device)\r\n",
        "  prediction = model(node_sets, neighbor_sets, public_edge_masks)\r\n",
        "  test_preds.append(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABuUSabmQ4qw"
      },
      "source": [
        "\"\"\"model_save_name = 'Tt_lvl_company description.pt'\r\n",
        "path = F\"/content/gdrive/My Drive/GNN Intern/saved_models/{model_save_name}\"\r\n",
        "torch.save(model.state_dict(),path)\r\n",
        "# model has lot of parameters, so saving is not possible with current colab runtime\r\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfFSmbFlsS_s"
      },
      "source": [
        "# Saving Train, Validation, Test Predictions and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8NR-dQUmmjN"
      },
      "source": [
        "train_labels = [label.cpu().data.numpy() for label in train_labels]\r\n",
        "train_labels = np.concatenate(train_labels)\r\n",
        "\r\n",
        "validation_labels = [label.cpu().data.numpy() for label in validation_labels]\r\n",
        "validation_labels = np.concatenate(validation_labels)\r\n",
        "\r\n",
        "np.savez('/content/gdrive/My Drive/GNN Intern/labels.npz',train_labels = train_labels, validation_labels = validation_labels)\r\n",
        "del train_labels, validation_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UJIrYCFkqFr"
      },
      "source": [
        "train_preds = [pred.cpu().data.numpy() for pred in train_preds]\r\n",
        "train_preds = np.vstack(train_preds)\r\n",
        "\r\n",
        "validation_preds = [pred.cpu().data.numpy() for pred in validation_preds]\r\n",
        "validation_preds = np.vstack(validation_preds)\r\n",
        "\r\n",
        "test_preds = [i.cpu().data.numpy() for i in test_preds]\r\n",
        "test_preds = np.vstack(test_preds)\r\n",
        "\r\n",
        "np.savez('/content/gdrive/My Drive/GNN Intern/predictions.npz',train_preds = train_preds, validation_preds = validation_preds, test_preds = test_preds)\r\n",
        "del train_preds, validation_preds, test_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOe2Y-iGeA5U"
      },
      "source": [
        "# MRR, F1 and ROC_AUC Scores for Train and Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3MYui20rhNb"
      },
      "source": [
        "labels = np.load('/content/gdrive/My Drive/GNN Intern/labels.npz')\r\n",
        "preds = np.load('/content/gdrive/My Drive/GNN Intern/predictions.npz')\r\n",
        "\r\n",
        "train_preds = preds['train_preds']\r\n",
        "validation_preds = preds['validation_preds']\r\n",
        "\r\n",
        "train_labels = labels['train_labels']\r\n",
        "validation_labels = labels['validation_labels']\r\n",
        "\r\n",
        "train_preds = train_preds.argmax(axis=1)\r\n",
        "validation_preds = validation_preds.argmax(axis=1)\r\n",
        "\r\n",
        "train_f1 = f1_score(train_labels,train_preds,average='weighted')\r\n",
        "validation_f1 = f1_score(validation_labels,validation_preds,average='weighted')\r\n",
        "print(\"F1_score of train_set: \",train_f1, \" F1_score of validation_set: \",validation_f1)\r\n",
        "\r\n",
        "#Mean Reciprocal Rank (MRR)\r\n",
        "train_mrr = [np.array(arr).argsort()[::-1] for arr in preds[\"train_preds\"]]\r\n",
        "validation_mrr = [np.array(arr).argsort()[::-1] for arr in preds['validation_preds']]\r\n",
        "t_mrr = np.mean([1/(1 + np.where(arr == i)[0].item()) for arr,i in zip(train_mrr,train_labels)])\r\n",
        "v_mrr = np.mean([1/(1+np.where(arr == i)[0].item()) for arr,i in zip(validation_mrr,validation_labels)])\r\n",
        "print(\"MRR of train_set: \",t_mrr,\" MRR of validation_set\",v_mrr)\r\n",
        "\r\n",
        "train_roc_ovr = roc_auc_score(train_labels,preds['train_preds'],multi_class='ovr',average='weighted')\r\n",
        "train_roc_ovo = roc_auc_score(train_labels,preds['train_preds'],multi_class='ovo',average='weighted')\r\n",
        "validation_roc_ovr = roc_auc_score(validation_labels,preds['validation_preds'],multi_class='ovr',average='weighted')\r\n",
        "validation_roc_ovo = roc_auc_score(validation_labels,preds['validation_preds'],multi_class='ovo',average='weighted')\r\n",
        "print(\"Area under ROC curve:\\n\\t one versus rest weighted average of AUC:\")\r\n",
        "print(\"\\t AUC of train_set: \",train_roc_ovr, \"AUC of validation_set: \",validation_roc_ovr)\r\n",
        "print(\"\\t one versus one weighted average of AUC: \")\r\n",
        "print(\"\\t AUC of train_set: \",train_roc_ovo, \"AUC of validation_set: \",validation_roc_ovo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9oxoQ26CVyI"
      },
      "source": [
        "# Output for Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64eWOJ4vCSW5"
      },
      "source": [
        "#upto 5 tags for each description\r\n",
        "test_args = [np.array(arr).argsort()[::-1][:5] for arr in preds[\"test_preds\"]]\r\n",
        "df = pd.read_excel('/content/gdrive/MyDrive/GNN Intern/Testing_Data.xlsx')\r\n",
        "df['First order tag'] = [classes[str(tags[0])] for tags in test_args]\r\n",
        "df['Second order tag'] = [classes[str(tags[1])] for tags in test_args]\r\n",
        "df['Third order tag'] = [classes[str(tags[2])] for tags in test_args]\r\n",
        "df['Fourth order tag'] = [classes[str(tags[3])] for tags in test_args]\r\n",
        "df['Fifth order tag'] = [classes[str(tags[4])] for tags in test_args]\r\n",
        "df.to_csv('/content/gdrive/MyDrive/GNN Intern/Output.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}